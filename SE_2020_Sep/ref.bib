
@article{dam_deep_2018,
	title = {A deep tree-based model for software defect prediction},
	abstract = {Defects are common in software systems and can potentially cause various problems to software users. Different methods have been developed to quickly predict the most likely locations of defects in large code bases. Most of them focus on designing features (e.g. complexity metrics) that correlate with potentially defective code. Those approaches however do not sufficiently capture the syntax and different levels of semantics of source code, an important capability for building accurate prediction models. In this paper, we develop a novel prediction model which is capable of automatically learning features for representing source code and using them for defect prediction. Our prediction system is built upon the powerful deep learning, tree-structured Long Short Term Memory network which directly matches with the Abstract Syntax Tree representation of source code. An evaluation on two datasets, one from open source projects contributed by Samsung and the other from the public PROMISE repository, demonstrates the effectiveness of our approach for both within-project and cross-project predictions.},
	journal = {ArXiv},
	author = {Dam, Khanh Hoa and Pham, T. and Ng, S. W. and Tran, T. and Grundy, J. and Ghose, A. and Kim, T. and Kim, C.},
	year = {2018},
	file = {Full Text PDF:/home/rem/Zotero/storage/K9BF9WIW/Dam 等。 - 2018 - A deep tree-based model for software defect predic.pdf:application/pdf}
}

@inproceedings{yang_defect_2016,
	title = {Defect {Prediction} on {Unlabeled} {Datasets} by {Using} {Unsupervised} {Clustering}},
	doi = {10.1109/HPCC-SmartCity-DSS.2016.0073},
	abstract = {Defect prediction on unlabeled datasets is one of the most active research areas in software engineering. Generally, cross-project defect prediction (CPDP) and unsupervised learning defect prediction are utilized to address this problem. The fundamental idea of CPDP is the transfer learning that reuses the prediction model built by labeled source projects. However, because of the difference of data distribution among projects, the prediction performance of CPDP models varies by projects. Usually, unsupervised learning models are not comparable to supervised learning ones in term of prediction performance. Hence, many unsupervised prediction models require manual effort to achieve good prediction performance. Recently, a novel unsupervised learning approach, which is without manual effort and based upon the magnitude of metric value, has been proposed and got good prediction performance on some datasets. With the heuristic of this approach, this paper proposes a new approach for predicting defect proneness on unlabeled datasets-ACL. In our empirical study on 16 open source projects, the ACL models led prediction performance on 9 datasets in term of F-measure, which are comparable to supervised learning models in term of predictive power.},
	booktitle = {2016 {IEEE} 18th {International} {Conference} on {High} {Performance} {Computing} and {Communications}; {IEEE} 14th {International} {Conference} on {Smart} {City}; {IEEE} 2nd {International} {Conference} on {Data} {Science} and {Systems} ({HPCC}/{SmartCity}/{DSS})},
	author = {Yang, Jun and Qian, Hongbing},
	month = dec,
	year = {2016},
	keywords = {public domain software, CPDP, cross-project defect prediction, data distribution, data handling, Defect Prediction, Labeling, Measurement, open source project, pattern clustering, Prediction algorithms, Predictive models, Software, software engineering, software fault tolerance, Supervised learning, transfer learning, unlabeled dataset, Unlabeled Datasets, Unsupervised Cluetring, unsupervised clustering, unsupervised learning, Unsupervised learning},
	pages = {465--472},
	file = {IEEE Xplore Abstract Record:/home/rem/Zotero/storage/48XDLQ74/7828414.html:text/html}
}

@inproceedings{yang_deep_2015,
	title = {Deep {Learning} for {Just}-in-{Time} {Defect} {Prediction}},
	doi = {10.1109/QRS.2015.14},
	abstract = {Defect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time. Nowadays, deep learning is a hot topic in the machine learning literature. Whether deep learning can be used to improve the performance of just-in-time defect prediction is still uninvestigated. In this paper, to bridge this research gap, we propose an approach Deeper which leverages deep learning techniques to predict defect-prone changes. We first build a set of expressive features from a set of initial change features by leveraging a deep belief network algorithm. Next, a machine learning classifier is built on the selected features. To evaluate the performance of our approach, we use datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. We compare our approach with the approach proposed by Kamei et al. The experimental results show that on average across the 6 projects, Deeper could discover 32.22\% more bugs than Kamei et al's approach (51.04\% versus 18.82\% on average). In addition, Deeper can achieve F1-scores of 0.22-0.63, which are statistically significantly higher than those of Kamei et al.'s approach on 4 out of the 6 projects.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Software} {Quality}, {Reliability} and {Security}},
	author = {Yang, Xinli and Lo, David and Xia, Xin and Zhang, Yun and Sun, Jianling},
	month = aug,
	year = {2015},
	keywords = {learning (artificial intelligence), software quality, Measurement, change-level defect prediction, Computer bugs, Cost Effectiveness, Deep Belief Network, deep learning, Deep Learning, Feature extraction, just-in-time, just-in-time defect prediction, Just-In-Time Defect Prediction, Logistics, Machine learning, machine learning classifier, machine learning literature, pattern classification, Software quality, Training},
	pages = {17--26},
	file = {IEEE Xplore Abstract Record:/home/rem/Zotero/storage/MH2P59MN/7272910.html:text/html;送出的版本:/home/rem/Zotero/storage/TG37QZ58/Yang 等。 - 2015 - Deep Learning for Just-in-Time Defect Prediction.pdf:application/pdf}
}

@article{charles__2017,
	title = {机器学习证明了它在商业上的价值},
	language = {中文;},
	urldate = {2020-09-18},
	journal = {计算机世界},
	author = {charles, Bob  Violino  纽约《计算机世界》特约撰稿人  编译},
	month = apr,
	year = {2017},
	note = {Publisher: 计算机世界},
	pages = {002}
}

@article{__2020,
	title = {基于机器学习的虚拟仪器软件缺陷预测模型研究},
	abstract = {针对传统虚拟仪器软件预测模型易产生偏向无缺陷类别结果,导致出现缺陷预测性能较低的问题,提出基于机器学习的虚拟仪器软件缺陷预测模型。利用邻域清除算法进行数据预处理,消除重叠非缺陷样本,通过随机采样的方式,划分平衡训练集,使用朴素贝叶斯NB算法将训练集映射给定测试样本集中进行软件缺陷预测,得到多个缺陷预测子集,利用机器学习集成得到最终软件缺陷预测模型,完成基于机器学习的虚拟仪器软件缺陷预测模型构建。对比实验结果显示,设计软件缺陷预测模型在不同标记比例的9个程度不同不平衡数据集上,平均赢得的数据集数目为7.67,AUC值均较传统缺陷预测模型有较明显地提高,表明设计软件缺陷预测模型在不同软件数据集上有...},
	number = {05},
	journal = {自动化与仪器仪表},
	author = {曾路 and 汪浩},
	year = {2020},
	note = {ISBN: 1001-9227},
	keywords = {机器学习;虚拟仪器软件;软件缺陷预测;预测模型},
	pages = {59--62}
}

@article{song_general_2011,
	title = {A {General} {Software} {Defect}-{Proneness} {Prediction} {Framework}},
	volume = {37},
	issn = {1939-3520},
	doi = {10.1109/TSE.2010.90},
	abstract = {BACKGROUND - Predicting defect-prone software components is an economically important activity and so has received a good deal of attention. However, making sense of the many, and sometimes seemingly inconsistent, results is difficult. OBJECTIVE - We propose and evaluate a general framework for software defect prediction that supports 1) unbiased and 2) comprehensive comparison between competing prediction systems. METHOD - The framework is comprised of 1) scheme evaluation and 2) defect prediction components. The scheme evaluation analyzes the prediction performance of competing learning schemes for given historical data sets. The defect predictor builds models according to the evaluated learning scheme and predicts software defects with new data according to the constructed model. In order to demonstrate the performance of the proposed framework, we use both simulation and publicly available software defect data sets. RESULTS - The results show that we should choose different learning schemes for different data sets (i.e., no scheme dominates), that small details in conducting how evaluations are conducted can completely reverse findings, and last, that our proposed framework is more effective and less prone to bias than previous approaches. CONCLUSIONS - Failure to properly or fully evaluate a learning scheme can be misleading; however, these problems may be overcome by our proposed framework.},
	number = {3},
	journal = {IEEE Transactions on Software Engineering},
	author = {Song, Qinbao and Jia, Zihan and Shepperd, Martin and Ying, Shi and Liu, Jin},
	month = may,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {learning (artificial intelligence), Prediction algorithms, Predictive models, Software, software fault tolerance, Training, Buildings, competing learning schemes, Data models, defect predictor, machine learning, scheme evaluation, scheme evaluation., Software defect prediction, software defect proneness prediction framework, software defect-proneness prediction, software performance evaluation, Training data},
	pages = {356--370},
	file = {IEEE Xplore Abstract Record:/home/rem/Zotero/storage/K24TUIFZ/5611551.html:text/html;接受的版本:/home/rem/Zotero/storage/46365TS2/Song 等。 - 2011 - A General Software Defect-Proneness Prediction Fra.pdf:application/pdf}
}

@phdthesis{__2017,
	type = {硕士},
	title = {基于深度学习的软件缺陷预测技术研究},
	abstract = {由于软件的复杂度不断提高,软件中存在的缺陷不断增多,由这些软件缺陷引发的灾难屡见不鲜,因此,为了减少软件灾难的发生,软件缺陷预测技术成为计算机科学技术领域研究热点之一。在构建软件缺陷预测模型过程中,特征选择算法对于构建的软件缺陷预测模型的精准度有很大的影响,而传统特征选择算法如:PCA以及LDA等在对数据特征进行深度学习以及抵抗噪声和缺失值的干扰等方面存在很大的局限性。本文针对这些问题提出了利用两种深度学习算法来构建软件缺陷预测模型,同时,提出了一种软件缺陷预测模型构建的完整框架,并基于该框架研发了软件缺陷预测系统。论文主要工作如下:(1)针对传统PCA以及LDA等特征选择方法无法获得特征之间的非线性特征关系问题提出利用深度信念网和支持向量机构建软件缺陷预测模型(DBN-SVM),通过仿真实验与PCA和LDA构建的支持向量机模型相比,DBN-SVM软件缺陷预测模型具有较高的预测精准度。(2)针对深度信念网无法消除数据中的噪声以及缺失值对软件缺陷预测模型的准确度的影响等问题,提出了利用降噪自动编码器和支持向量机来构建软件缺陷预测模型(DA-SVM),通过仿真实验与DBN-SVM相比,DA-SVM软件缺陷预测模型不仅提高了预测精准度,还增强了模型的鲁棒性。(3)由于软件缺陷预测模型的预测准确性不仅受特征选择方法的影响,数据预处理以及学习算法同样是影响软件缺陷预测模型准确性的重要因素,因此,针对不同学习规则构建的软件缺陷预测模型性能的差异性,提出了含有数据预处理、特征选择以及学习算法等处理方法的软件缺陷预测框架,并基于该框架开发了软件缺陷预测系统。通过软件缺陷预测系统可以针对数据集筛选最佳的学习规则来构建软件缺陷预测模型并进行软件缺陷预测工作。},
	school = {南京航空航天大学},
	author = {甘露 and 臧洌},
	year = {2017},
	keywords = {软件缺陷预测;深度信念网;支持向量机;降噪自动编码器;特征选择}
}

@phdthesis{__2017-1,
	type = {硕士},
	title = {基于特征选择和集成学习的软件缺陷预测技术研究},
	abstract = {随着软件系统规模和应用范围的扩大,软件缺陷引发的产品故障越来越多。如何提高软件质量成为软件工程领域一个重要的研究方向。机器学习与数据挖掘的方法可以用于预测软件模块中是否存在缺陷,从而进一步定位缺陷,提高软件质量。软件缺陷预测的关键在于提取对象信息中的特征信息,然后利用这些特征信息进行建模分析来预测软件缺陷。然而在实际应用场景中,每个特征对缺陷预测的贡献不尽相同。为了降低运算复杂度、提高预测精度,研究者们通常进行特征选择;另外,针对软件缺陷定位精度不足的问题,通过对比多种分类器的表现,本文引入了集成学习方法。主要开展的研究工作有:第一,本文结合层次聚类和特征排序方法,提出一种面向软件缺陷预测的特...},
	language = {中文;},
	urldate = {2020-10-09},
	school = {杭州电子科技大学},
	author = {程, 辉},
	year = {2017},
	keywords = {分类器, 层次聚类, 软件缺陷预测, 特征选择, 集成学习, Classification, Ensemble Learning, Feature Selection, Hierarchical Clustering, Software Defect Prediction}
}

@article{dam_lessons_2019-1,
	title = {Lessons {Learned} from {Using} a {Deep} {Tree}-{Based} {Model} for {Software} {Defect} {Prediction} in {Practice}},
	doi = {10.1109/MSR.2019.00017},
	abstract = {Defects are common in software systems and cause many problems for software users. Different methods have been developed to make early prediction about the most likely defective modules in large codebases. Most focus on designing features (e.g. complexity metrics) that correlate with potentially defective code. Those approaches however do not sufficiently capture the syntax and multiple levels of semantics of source code, a potentially important capability for building accurate prediction models. In this paper, we report on our experience of deploying a new deep learning tree-based defect prediction model in practice. This model is built upon the tree-structured Long Short Term Memory network which directly matches with the Abstract Syntax Tree representation of source code. We discuss a number of lessons learned from developing the model and evaluating it on two datasets, one from open source projects contributed by our industry partner Samsung and the other from the public PROMISE repository.},
	journal = {2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)},
	author = {Dam, Khanh Hoa and Pham, T. and Ng, S. W. and Tran, T. and Grundy, J. and Ghose, A. and Kim, T. and Kim, Chul-Joo},
	year = {2019}
}

@article{dam_automatic_2017,
	title = {Automatic feature learning for vulnerability prediction},
	abstract = {Code flaws or vulnerabilities are prevalent in software systems and can potentially cause a variety of problems including deadlock, information loss, or system failure. A variety of approaches have been developed to try and detect the most likely locations of such code vulnerabilities in large code bases. Most of them rely on manually designing features (e.g. complexity metrics or frequencies of code tokens) that represent the characteristics of the code. However, all suffer from challenges in sufficiently capturing both semantic and syntactic representation of source code, an important capability for building accurate prediction models. In this paper, we describe a new approach, built upon the powerful deep learning Long Short Term Memory model, to automatically learn both semantic and syntactic features in code. Our evaluation on 18 Android applications demonstrates that the prediction power obtained from our learned features is equal or even superior to what is achieved by state of the art vulnerability prediction models: 3\%--58\% improvement for within-project prediction and 85\% for cross-project prediction.},
	journal = {ArXiv},
	author = {Dam, Khanh Hoa and Tran, T. and Pham, T. and Ng, S. W. and Grundy, J. and Ghose, A.},
	year = {2017},
	file = {Full Text PDF:/home/rem/Zotero/storage/N9ICEJAR/Dam 等。 - 2017 - Automatic feature learning for vulnerability predi.pdf:application/pdf}
}

@misc{noauthor_pdf_nodate,
	title = {[{PDF}] {How} {Well} {Do} {Change} {Sequences} {Predict} {Defects}? {Sequence} {Learning} from {Software} {Changes} {\textbar} {Semantic} {Scholar}},
	urldate = {2020-10-09},
	file = {[PDF] How Well Do Change Sequences Predict Defects? Sequence Learning from Software Changes | Semantic Scholar:/home/rem/Zotero/storage/5QBNHXBX/86b822d4f08013acffbf3af75910eec5d754243c.html:text/html}
}

@article{wen_how_2018,
	title = {How {Well} {Do} {Change} {Sequences} {Predict} {Defects}? {Sequence} {Learning} from {Software} {Changes}},
	shorttitle = {How {Well} {Do} {Change} {Sequences} {Predict} {Defects}?},
	doi = {10.1109/TSE.2018.2876256},
	abstract = {Software defect prediction, which aims to identify defective modules, can assist developers in finding bugs and prioritizing limited quality assurance resources. Various features to build defect prediction models have been proposed and evaluated. Among them, process metrics are one important category. Yet, existing process metrics are mainly encoded manually from change histories and ignore the sequential information arising from the changes during software evolution. Are the change sequences derived from such information useful to characterize buggy program modules? How can we leverage such sequences to build good defect prediction models? Unlike traditional process metrics used for existing defect prediction models, change sequences are mostly vectors of variable length. This makes it difficult to apply such sequences directly in prediction models that are driven by conventional classifiers. To resolve this challenge, we utilize Recurrent Neural Network (RNN), which is a deep learning technique, to encode features from sequence data automatically. In this paper, we propose a novel approach called FENCES, which extracts six types of change sequences covering different aspects of software changes via fine-grained change analysis. It approaches defects prediction by mapping it to a sequence labeling problem solvable by RNN. Our evaluations on 10 open source projects show that FENCES can predict defects with high performance. In particular, our approach achieves an average F-measure of 0.657, which improves the prediction models built on traditional metrics significantly. The improvements vary from 31.6\% to 46.8\% on average. In terms of AUC, FENCES achieves an average value of 0.892, and the improvements over baselines vary from 4.2\% to 16.1\%. FENCES also outperforms the state-of-the-art technique which learns semantic features automatically from static code via deep learning.},
	author = {Wen, M. and Wu, Rongxin and Cheung, S.},
	year = {2018},
	file = {Full Text PDF:/home/rem/Zotero/storage/RUSXJF9S/Wen 等。 - 2018 - How Well Do Change Sequences Predict Defects Sequ.pdf:application/pdf}
}

@article{li_software_2017,
	title = {Software {Defect} {Prediction} via {Convolutional} {Neural} {Network}},
	doi = {10.1109/QRS.2017.42},
	abstract = {To improve software reliability, software defect prediction is utilized to assist developers in finding potential bugs and allocating their testing efforts. Traditional defect prediction studies mainly focus on designing hand-crafted features, which are input into machine learning classifiers to identify defective code. However, these hand-crafted features often fail to capture the semantic and structural information of programs. Such information is important in modeling program functionality and can lead to more accurate defect prediction.In this paper, we propose a framework called Defect Prediction via Convolutional Neural Network (DP-CNN), which leverages deep learning for effective feature generation. Specifically, based on the programs' Abstract Syntax Trees (ASTs), we first extract token vectors, which are then encoded as numerical vectors via mapping and word embedding. We feed the numerical vectors into Convolutional Neural Network to automatically learn semantic and structural features of programs. After that, we combine the learned features with traditional hand-crafted features, for accurate software defect prediction. We evaluate our method on seven open source projects in terms of F-measure in defect prediction. The experimental results show that in average, DP-CNN improves the state-of-the-art method by 12\%.},
	journal = {2017 IEEE International Conference on Software Quality, Reliability and Security (QRS)},
	author = {Li, Jian and He, Pinjia and Zhu, J. and Lyu, M.},
	year = {2017}
}

@article{wang_automatically_2016,
	title = {Automatically {Learning} {Semantic} {Features} for {Defect} {Prediction}},
	doi = {10.1145/2884781.2884804},
	abstract = {Software defect prediction, which predicts defective code regions, can help developers find bugs and prioritize their testing efforts. To build accurate prediction models, previous studies focus on manually designing features that encode the characteristics of programs and exploring different machine learning algorithms. Existing traditional features often fail to capture the semantic differences of programs, and such a capability is needed for building accurate prediction models. To bridge the gap between programs' semantics and defect prediction features, this paper proposes to leverage a powerful representation-learning algorithm, deep learning, to learn semantic representation of programs automatically from source code. Specifically, we leverage Deep Belief Network (DBN) to automatically learn semantic features from token vectors extracted from programs' Abstract Syntax Trees (ASTs). Our evaluation on ten open source projects shows that our automatically learned semantic features significantly improve both within-project defect prediction (WPDP) and cross-project defect prediction (CPDP) compared to traditional features. Our semantic features improve WPDP on average by 14.7\% in precision, 11.5\% in recall, and 14.2\% in F1. For CPDP, our semantic features based approach outperforms the state-of-the-art technique TCA+ with traditional features by 8.9\% in F1.},
	journal = {2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)},
	author = {Wang, S. and Liu, Taiyue and Tan, Lin},
	year = {2016}
}

@article{yang_tlel_2017,
	title = {{TLEL}: {A} two-layer ensemble learning approach for just-in-time defect prediction},
	shorttitle = {{TLEL}},
	doi = {10.1016/j.infsof.2017.03.007},
	abstract = {Abstract Context Defect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time [1]. Objective Ensemble learning becomes a hot topic in recent years. There have been several studies about applying ensemble learning to defect prediction [2–5]. Traditional ensemble learning approaches only have one layer, i.e., they use ensemble learning once. There are few studies that leverages ensemble learning twice or more. To bridge this research gap, we try to hybridize various ensemble learning methods to see if it will improve the performance of just-in-time defect prediction. In particular, we focus on one way to do this by hybridizing bagging and stacking together and leave other possibly hybridization strategies for future work. Method In this paper, we propose a two-layer ensemble learning approach TLEL which leverages decision tree and ensemble learning to improve the performance of just-in-time defect prediction. In the inner layer, we combine decision tree and bagging to build a Random Forest model. In the outer layer, we use random under-sampling to train many different Random Forest models and use stacking to ensemble them once more. Results To evaluate the performance of TLEL, we use two metrics, i.e., cost effectiveness and F1-score. We perform experiments on the datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. Also, we compare our approach with three baselines, i.e., Deeper, the approach proposed by us [6], DNC, the approach proposed by Wang et al. [2], and MKEL, the approach proposed by Wang et al. [3]. The experimental results show that on average across the six datasets, TLEL could discover over 70\% of the bugs by reviewing only 20\% of the lines of code, as compared with about 50\% for the baselines. In addition, the F1-scores TLEL can achieve are substantially and statistically significantly higher than those of three baselines across the six datasets. Conclusion TLEL can achieve a substantial and statistically significant improvement over the state-of-the-art methods, i.e., Deeper, DNC and MKEL. Moreover, TLEL could discover over 70\% of the bugs by reviewing only 20\% of the lines of code.},
	journal = {Inf. Softw. Technol.},
	author = {Yang, Xinli and Lo, D. and Xia, Xin and Sun, Jianling},
	year = {2017}
}

@article{zhao_seqfuzzer_2019,
	title = {{SeqFuzzer}: {An} {Industrial} {Protocol} {Fuzzing} {Framework} from a {Deep} {Learning} {Perspective}},
	shorttitle = {{SeqFuzzer}},
	doi = {10.1109/ICST.2019.00016},
	abstract = {Industrial networks are the cornerstone of modern industrial control systems. Performing security checks of industrial communication processes helps detect unknown risks and vulnerabilities. Fuzz testing is a widely used method for performing security checks that takes advantage of automation. However, there is a big challenge to carry out security checks on industrial network due to the increasing variety and complexity of industrial communication protocols. In this case, existing approaches usually take a long time to model the protocol for generating test cases, which is labor-intensive and time-consuming. This becomes even worse when the target protocol is stateful. To help in addressing this problem, we employed a deep learning model to learn the structures of protocol frames and deal with the temporal features of stateful protocols. We propose a fuzzing framework named SeqFuzzer which automatically learns the protocol frame structures from communication traffic and generates fake but plausible messages as test cases. For proving the usability of our approach, we applied SeqFuzzer to widely-used Ethernet for Control Automation Technology (EtherCAT) devices and successfully detected several security vulnerabilities.},
	journal = {2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)},
	author = {Zhao, Hui and Li, Zhihui and Wei, Hansheng and Shi, Jianqi and Huang, Y.},
	year = {2019}
}

@inproceedings{fukushima_empirical_2014,
	title = {An empirical study of just-in-time defect prediction using cross-project models},
	doi = {10.1145/2597073.2597075},
	abstract = {Prior research suggests that predicting defect-inducing changes, i.e., Just-In-Time (JIT) defect prediction is a more practical alternative to traditional defect prediction techniques, providing immediate feedback while design decisions are still fresh in the minds of developers. Unfortunately, similar to traditional defect prediction models, JIT models require a large amount of training data, which is not available when projects are in initial development phases. To address this flaw in traditional defect prediction, prior work has proposed cross-project models, i.e., models learned from older projects with sufficient history. However, cross-project models have not yet been explored in the context of JIT prediction. Therefore, in this study, we empirically evaluate the performance of JIT cross-project models. Through a case study on 11 open source projects, we find that in a JIT cross-project context: (1) high performance within-project models rarely perform well; (2) models trained on projects that have similar correlations between predictor and dependent variables often perform well; and (3) ensemble learning techniques that leverage historical data from several other projects (e.g., voting experts) often perform well. Our findings empirically confirm that JIT cross-project models learned using other projects are a viable solution for projects with little historical data. However, JIT cross-project models perform best when the data used to learn them is carefully selected.},
	booktitle = {{MSR} 2014},
	author = {Fukushima, T. and Kamei, Yasutaka and McIntosh, Shane and Yamashita, K. and Ubayashi, N.},
	year = {2014}
}

@article{__2017-2,
	title = {基于多核字典学习的软件缺陷预测},
	volume = {44},
	issn = {1002-137X},
	abstract = {提出一种多核字典学习方法,用以对软件模块是否存在缺陷进行预测。用于软件缺陷预测的历史数据具有结构复杂、类不平衡的特点,用多个核函数构成的合成核将这些数据映射到一个高维特征空间,通过对多核字典基的选择,得到一个类别平衡的多核字典,用以对新的软件模块进行分类和预测,并判定其中是否存在缺陷。在NASA MDP数据集上的实验表明,与其他软件缺陷预测方法相比,多核字典学习方法能够针对软件缺陷历史数据结构复杂、类不平衡的特点,较好地解决软件缺陷预测问题。},
	language = {中文;},
	number = {12},
	urldate = {2020-10-09},
	journal = {计算机科学},
	author = {王, 铁建 and 吴, 飞 and 荆, 晓远},
	year = {2017},
	keywords = {多核学习, 字典学习, 软件缺陷预测, 类不平衡, Class-imbalance, Dictionary learning, Multiple kernel learning, Software detect prediction},
	pages = {131--134+168}
}

@misc{noauthor_bp--2015_nodate,
	title = {{基于改进BP算法的软件缺陷预测模型研究}--《北京理工大学》2015年硕士论文},
	file = {基于改进BP算法的软件缺陷预测模型研究--《北京理工大学》2015年硕士论文:/home/rem/Zotero/storage/3THF9PNQ/CDMD-10007-1015810001.html:text/html}
}

@article{__2018,
	title = {基于改进蝙蝠算法的软件缺陷预测模型},
	volume = {28},
	issn = {1673-629X},
	abstract = {软件缺陷预测模型因为软件规模持续扩大以及安全性要求越来越高,变得越来越重要。支持向量机(SVM)模型突出优点是它具有较强的非线性分类能力,所以在软件缺陷预测应用非常广泛。但是,SVM模型缺乏有效的方法来确定最佳参数,以至于不能达到理想的准确度。所以,提高SVM模型的参数,提高SVM模型的软件缺陷预测能力成为了研究热点。蝙蝠算法是一种启发式搜索算法,它模型简单,易于实现,但是却易陷入局部最优,因此采用加入莱维飞行的蝙蝠算法对SVM模型的参数选择进行优化。为了测试这个新模型的性能,仿真实验使用了一些软件缺陷预测的公共数据集,然后将结果与传统的启发式算法进行比较。实验结果表明,LBA-SVM模型的分...},
	language = {中文;},
	number = {12},
	journal = {计算机技术与发展},
	author = {杨, 晓琴},
	year = {2018},
	keywords = {支持向量机, 软件缺陷预测, 莱维飞行, 蝙蝠算法, bat algorithm, Levy flight, software defect prediction, support vector machine},
	pages = {74--78}
}

@misc{noauthor__nodate,
	title = {基于机器学习的软件缺陷预测 - 中国优秀硕士学位论文全文数据库},
	file = {基于机器学习的软件缺陷预测 - 中国优秀硕士学位论文全文数据库:/home/rem/Zotero/storage/JJ7BF5T6/detailall.html:text/html}
}

@misc{noauthor__nodate-2,
	title = {基于机器学习的软件缺陷预测 - 中国优秀硕士学位论文全文数据库},
	file = {基于机器学习的软件缺陷预测 - 中国优秀硕士学位论文全文数据库:/home/rem/Zotero/storage/2ZEMSU42/detailall.html:text/html}
}

@article{kamei_studying_2015,
	title = {Studying just-in-time defect prediction using cross-project models},
	doi = {10.1007/s10664-015-9400-x},
	abstract = {Unlike traditional defect prediction models that identify defect-prone modules, Just-In-Time (JIT) defect prediction models identify defect-inducing changes. As such, JIT defect models can provide earlier feedback for developers, while design decisions are still fresh in their minds. Unfortunately, similar to traditional defect models, JIT models require a large amount of training data, which is not available when projects are in initial development phases. To address this limitation in traditional defect prediction, prior work has proposed cross-project models, i.e., models learned from other projects with sufficient history. However, cross-project models have not yet been explored in the context of JIT prediction. Therefore, in this study, we empirically evaluate the performance of JIT models in a cross-project context. Through an empirical study on 11 open source projects, we find that while JIT models rarely perform well in a cross-project context, their performance tends to improve when using approaches that: (1) select models trained using other projects that are similar to the testing project, (2) combine the data of several other projects to produce a larger pool of training data, and (3) combine the models of several other projects to produce an ensemble model. Our findings empirically confirm that JIT models learned using other projects are a viable solution for projects with limited historical data. However, JIT models tend to perform best in a cross-project context when the data used to learn them are carefully selected.},
	journal = {Empirical Software Engineering},
	author = {Kamei, Yasutaka and Fukushima, T. and McIntosh, Shane and Yamashita, K. and Ubayashi, N. and Hassan, A.},
	year = {2015}
}

@article{xia_hydra_2016,
	title = {{HYDRA}: {Massively} {Compositional} {Model} for {Cross}-{Project} {Defect} {Prediction}},
	shorttitle = {{HYDRA}},
	doi = {10.1109/TSE.2016.2543218},
	abstract = {Most software defect prediction approaches are trained and applied on data from the same project. However, often a new project does not have enough training data. Cross-project defect prediction, which uses data from other projects to predict defects in a particular project, provides a new perspective to defect prediction. In this work, we propose a HYbrid moDel Reconstruction Approach (HYDRA) for cross-project defect prediction, which includes two phases: genetic algorithm (GA) phase and ensemble learning (EL) phase. These two phases create a massive composition of classifiers. To examine the benefits of HYDRA, we perform experiments on 29 datasets from the PROMISE repository which contains a total of 11,196 instances (i.e., Java classes) labeled as defective or clean. We experiment with logistic regression as the underlying classification algorithm of HYDRA. We compare our approach with the most recently proposed cross-project defect prediction approaches: TCA+ by Nam et al., Peters filter by Peters et al., GP by Liu et al., MO by Canfora et al., and CODEP by Panichella et al. Our results show that HYDRA achieves an average F1-score of 0.544. On average, across the 29 datasets, these results correspond to an improvement in the F1-scores of 26.22 , 34.99, 47.43, 28.61, and 30.14 percent over TCA+, Peters filter, GP, MO, and CODEP, respectively. In addition, HYDRA on average can discover 33 percent of all bugs if developers inspect the top 20 percent lines of code, which improves the best baseline approach (TCA+) by 44.41 percent. We also find that HYDRA improves the F1-score of Zero-R which predict all the instances to be defective by 5.42 percent, but improves Zero-R by 58.65 percent when inspecting the top 20 percent lines of code. In practice, Zero-R can be hard to use since it simply predicts all of the instances to be defective, and thus developers have to inspect all of the instances to find the defective ones. Moreover, we notice the improvement of HYDRA over other baseline approaches in terms of F1-score and when inspecting the top 20 percent lines of code are substantial, and in most cases the improvements are significant and have large effect sizes across the 29 datasets.},
	journal = {IEEE Transactions on Software Engineering},
	author = {Xia, Xin and Lo, D. and Pan, S. and Nagappan, N. and Wang, Xinyu},
	year = {2016}
}

@phdthesis{__2016,
	type = {博士},
	title = {基于机器学习的软件缺陷预测研究},
	abstract = {为开发高质量的软件并保障其可靠性,软件测试是软件开发过程中非常重要且不可缺少的阶段。然而随着当前软件在规模和复杂度上不断的提高,对软件进行全面的测试,以发现和修复隐藏的缺陷所耗费的成本也在迅速的增加。软件缺陷预测技术可以为测试人员提前定位软件中可能产生缺陷的模块,以指导决策人员分配有限的测试资源优先用于有缺陷模块的检测,在提高软件质量同时也节约了大量的时间和成本。本文主要针对基于机器学习的缺陷预测方法进行了讨论和研究,虽然先前的研究利用各种机器学习算法提出了许多有效的预测模型,但现有的方法在实际应用中仍存在着一些问题:(1)早期缺陷预测中缺乏足量的训练样本,可能导致传统的预测模型无法进行有效的...},
	language = {中文;},
	school = {重庆大学},
	author = {陈, 琳},
	year = {2016},
	keywords = {机器学习, 软件缺陷预测, 类不平衡的缺陷预测, 跨公司的缺陷预测, class imbalance defects prediction, cross-company defects prediction, machine learning, software defects prediction}
}

@article{lessmann_benchmarking_2008,
	title = {Benchmarking {Classification} {Models} for {Software} {Defect} {Prediction}: {A} {Proposed} {Framework} and {Novel} {Findings}},
	volume = {34},
	issn = {1939-3520},
	shorttitle = {Benchmarking {Classification} {Models} for {Software} {Defect} {Prediction}},
	doi = {10.1109/TSE.2008.35},
	abstract = {Software defect prediction strives to improve software quality and testing efficiency by constructing predictive classification models from code attributes to enable a timely identification of fault-prone modules. Several classification models have been evaluated for this task. However, due to inconsistent findings regarding the superiority of one classifier over another and the usefulness of metric-based classification in general, more research is needed to improve convergence across studies and further advance confidence in experimental results. We consider three potential sources for bias: comparing classifiers over one or a small number of proprietary data sets, relying on accuracy indicators that are conceptually inappropriate for software defect prediction and cross-study comparisons, and, finally, limited use of statistical testing procedures to secure empirical findings. To remedy these problems, a framework for comparative software defect prediction experiments is proposed and applied in a large-scale empirical comparison of 22 classifiers over 10 public domain data sets from the NASA Metrics Data repository. Overall, an appealing degree of predictive accuracy is observed, which supports the view that metric-based classification is useful. However, our results indicate that the importance of the particular classification algorithm may be less than previously assumed since no significant performance differences could be detected among the top 17 classifiers.},
	number = {4},
	journal = {IEEE Transactions on Software Engineering},
	author = {Lessmann, Stefan and Baesens, Bart and Mues, Christophe and Pietsch, Swantje},
	month = jul,
	year = {2008},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {benchmark testing, Benchmark testing, benchmarking classification models, code attributes, Complexity measures, Convergence, Data mining, Fault diagnosis, fault-prone modules, Formal methods, Large-scale systems, metric-based classification, NASA, predictive classification models, Predictive models, proprietary data sets, software defect prediction, software quality, Software quality, Software systems, Software testing, Statistical analysis, Statistical methods, statistical testing, statistical testing procedures, testing efficiency},
	pages = {485--496},
	file = {IEEE Xplore Abstract Record:/home/rem/Zotero/storage/89VJQ4CW/4527256.html:text/html;送出的版本:/home/rem/Zotero/storage/94HIFS3X/Lessmann 等。 - 2008 - Benchmarking Classification Models for Software De.pdf:application/pdf}
}

@inproceedings{hassan_predicting_2009,
	title = {Predicting faults using the complexity of code changes},
	doi = {10.1109/ICSE.2009.5070510},
	abstract = {Predicting the incidence of faults in code has been commonly associated with measuring complexity. In this paper, we propose complexity metrics that are based on the code change process instead of on the code. We conjecture that a complex code change process negatively affects its product, i.e., the software system. We validate our hypothesis empirically through a case study using data derived from the change history for six large open source projects. Our case study shows that our change complexity metrics are better predictors of fault potential in comparison to other well-known historical predictors of faults, i.e., prior modifications and prior faults.},
	booktitle = {2009 {IEEE} 31st {International} {Conference} on {Software} {Engineering}},
	author = {Hassan, Ahmed E.},
	month = may,
	year = {2009},
	note = {ISSN: 1558-1225},
	keywords = {code change process, code changes complexity, complexity measurement, complexity metrics, Delay, Entropy, fault incidence, fault prediction, History, Information theory, Lab-on-a-chip, Predictive models, program diagnostics, program verification, Project management, software fault tolerance, Software measurement, software metrics, Software systems},
	pages = {78--88},
	file = {IEEE Xplore Abstract Record:/home/rem/Zotero/storage/XUJVMVBU/5070510.html:text/html}
}

@article{menzies_local_2013,
	title = {Local versus {Global} {Lessons} for {Defect} {Prediction} and {Effort} {Estimation}},
	volume = {39},
	issn = {1939-3520},
	doi = {10.1109/TSE.2012.83},
	abstract = {Existing research is unclear on how to generate lessons learned for defect prediction and effort estimation. Should we seek lessons that are global to multiple projects or just local to particular projects? This paper aims to comparatively evaluate local versus global lessons learned for effort estimation and defect prediction. We applied automated clustering tools to effort and defect datasets from the PROMISE repository. Rule learners generated lessons learned from all the data, from local projects, or just from each cluster. The results indicate that the lessons learned after combining small parts of different data sources (i.e., the clusters) were superior to either generalizations formed over all the data or local lessons formed from particular projects. We conclude that when researchers attempt to draw lessons from some historical data source, they should 1) ignore any existing local divisions into multiple sources, 2) cluster across all available data, then 3) restrict the learning of lessons to the clusters from other sources that are nearest to the test data.},
	number = {6},
	journal = {IEEE Transactions on Software Engineering},
	author = {Menzies, Tim and Butcher, Andrew and Cok, David and Marcus, Andrian and Layman, Lucas and Shull, Forrest and Turhan, Burak and Zimmermann, Thomas},
	month = jun,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {automated clustering tools, automatic test pattern generation, clustering, Context, data mining, Data mining, Data models, data source, defect dataset, defect prediction, effort estimation, Estimation, global lessons, Java, learned lesson generated rule, local lessons, Measurement, pattern clustering, PROMISE repository, Software, Telecommunications},
	pages = {822--834},
	file = {IEEE Xplore Abstract Record:/home/rem/Zotero/storage/XRFIVU78/6363444.html:text/html}
}

@inproceedings{xia_predicting_2016,
	address = {New York, NY, USA},
	series = {{ESEM} '16},
	title = {Predicting {Crashing} {Releases} of {Mobile} {Applications}},
	isbn = {978-1-4503-4427-2},
	doi = {10.1145/2961111.2962606},
	abstract = {Context: The quality of mobile applications has a vital impact on their user's experience, ratings and ultimately overall success. Given the high competition in the mobile application market, i.e., many mobile applications perform the same or similar functionality, users of mobile apps tend to be less tolerant to quality issues. Goal: Therefore, identifying these crashing releases early on so that they can be avoided will help mobile app developers keep their user base and ensure the overall success of their apps. Method: To help mobile developers, we use machine learning techniques to effectively predict mobile app releases that are more likely to cause crashes, i.e., crashing releases. To perform our prediction, we mine and use a number of factors about the mobile releases, that are grouped into six unique dimensions: complexity, time, code, diffusion, commit, and text, and use a Naive Bayes classified to perform our prediction. Results: We perform an empirical study on 10 open source mobile applications containing a total of 2,638 releases from the F-Droid repository. On average, our approach can achieve F1 and AUC scores that improve over a baseline (random) predictor by 50\% and 28\%, respectively. We also find that factors related to text extracted from the commit logs prior to a release are the best predictors of crashing releases and have the largest effect. Conclusions: Our proposed approach could help to identify crash releases for mobile apps.},
	booktitle = {Proceedings of the 10th {ACM}/{IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}},
	publisher = {Association for Computing Machinery},
	author = {Xia, Xin and Shihab, Emad and Kamei, Yasutaka and Lo, David and Wang, Xinyu},
	month = sep,
	year = {2016},
	keywords = {Crash Release, Mobile Applications, Prediction Model},
	pages = {1--10}
}

@inproceedings{mende_revisiting_2009,
	address = {New York, NY, USA},
	series = {{PROMISE} '09},
	title = {Revisiting the evaluation of defect prediction models},
	isbn = {978-1-60558-634-2},
	doi = {10.1145/1540438.1540448},
	abstract = {Defect Prediction Models aim at identifying error-prone parts of a software system as early as possible. Many such models have been proposed, their evaluation, however, is still an open question, as recent publications show. An important aspect often ignored during evaluation is the effort reduction gained by using such models. Models are usually evaluated per module by performance measures used in information retrieval, such as recall, precision, or the area under the ROC curve (AUC). These measures assume that the costs associated with additional quality assurance activities are the same for each module, which is not reasonable in practice. For example, costs for unit testing and code reviews are roughly proportional to the size of a module. In this paper, we investigate this discrepancy using optimal and trivial models. We describe a trivial model that takes only the module size measured in lines of code into account, and compare it to five classification methods. The trivial model performs surprisingly well when evaluated using AUC. However, when an effort-sensitive performance measure is used, it becomes apparent that the trivial model is in fact the worst.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Predictor} {Models} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Mende, Thilo and Koschke, Rainer},
	month = may,
	year = {2009},
	keywords = {cost-sensitive performance measures, defect prediction},
	pages = {1--10}
}

@article{chen_drlgencert_2018,
	title = {{DRLgencert}: {Deep} {Learning}-{Based} {Automated} {Testing} of {Certificate} {Verification} in {SSL}/{TLS} {Implementations}},
	shorttitle = {{DRLgencert}},
	doi = {10.1109/ICSME.2018.00014},
	abstract = {The Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols are the foundation of network security. The certificate verification in SSL/TLS implementations is vital and may become the "weak link" in the whole network ecosystem. In previous works, some research focused on the automated testing of certificate verification, and the main approaches rely on generating massive certificates through randomly combining parts of seed certificates for fuzzing. Although the generated certificates could meet the semantic constraints, the cost is quite heavy, and the performance is limited due to the randomness. To fill this gap, in this paper, we propose DRLGENCERT, the first framework of applying deep reinforcement learning to the automated testing of certificate verification in SSL/TLS implementations. DRLGENCERT accepts ordinary certificates as input and outputs newly generated certificates which could trigger discrepancies with high efficiency. Benefited by the deep reinforcement learning, when generating certificates, our framework could choose the best next action according to the result of a previous modification, instead of simple random combinations. At the same time, we developed a set of new techniques to support the overall design, like new feature extraction method for X.509 certificates, fine-grained differential testing, and so forth. Also, we implemented a prototype of DRLGENCERT and carried out a series of real-world experiments. The results show DRLGENCERT is quite efficient, and we obtained 84,661 discrepancy-triggering certificates from 181,900 certificate seeds, say around 46.5\% effectiveness. Also, we evaluated six popular SSL/TLS implementations, including GnuTLS, MatrixSSL, MbedTLS, NSS, OpenSSL, and wolfSSL. DRLGENCERT successfully discovered 23 serious certificate verification flaws, and most of them were previously unknown.},
	journal = {2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
	author = {Chen, C. and Diao, Wenrui and Zeng, Yingpei and Guo, Shanqing and Hu, Chengyu},
	year = {2018},
	file = {送出的版本:/home/rem/Zotero/storage/Y7H9WBJP/Chen 等。 - 2018 - DRLgencert Deep Learning-Based Automated Testing .pdf:application/pdf}
}

@inproceedings{goel_literature_2017,
	title = {A literature review on cross project defect prediction},
	doi = {10.1109/UPCON.2017.8251131},
	abstract = {In the area of defect prediction most of the literature comprises of within project defect prediction. It is always not feasible to have the historical data of the similar projects for predictions. Therefore, CPDP (Cross Project Defect Prediction) as a subset of defect prediction in general has become a popular topic in research these days. In this paper, we present a systematic review of the CPDP. This paper summarizes the different methodologies used by various authors. A great amount of variation and heterogeneity is observed in the prediction process. The variations in the datasets, the learners, metric selection and the standard measure for comparisons are a challenge to determine the best practice for CPDP.},
	booktitle = {2017 4th {IEEE} {Uttar} {Pradesh} {Section} {International} {Conference} on {Electrical}, {Computer} and {Electronics} ({UPCON})},
	author = {Goel, Lipika and Damodaran, D. and Khatri, Sunil Kumar and Sharma, Mayank},
	month = oct,
	year = {2017},
	keywords = {Artificial neural networks, CPDP, cross project defect prediction, Logistics, Machine Learning, Measurement, metric selection, Object oriented modeling, Predictive models, Software, software metrics, software quality, Training, Transfer Learning},
	pages = {680--685},
	file = {IEEE Xplore Abstract Record:/home/rem/Zotero/storage/QZNZ4ELR/8251131.html:text/html}
}

@article{tantithamthavorn_impact_2019,
	title = {The {Impact} of {Automated} {Parameter} {Optimization} on {Defect} {Prediction} {Models}},
	volume = {45},
	issn = {1939-3520},
	doi = {10.1109/TSE.2018.2794977},
	abstract = {Defect prediction models-classifiers that identify defect-prone software modules-have configurable parameters that control their characteristics (e.g., the number of trees in a random forest). Recent studies show that these classifiers underperform when default settings are used. In this paper, we study the impact of automated parameter optimization on defect prediction models. Through a case study of 18 datasets, we find that automated parameter optimization: (1) improves AUC performance by up to 40 percentage points; (2) yields classifiers that are at least as stable as those trained using default settings; (3) substantially shifts the importance ranking of variables, with as few as 28 percent of the top-ranked variables in optimized classifiers also being top-ranked in non-optimized classifiers; (4) yields optimized settings for 17 of the 20 most sensitive parameters that transfer among datasets without a statistically significant drop in performance; and (5) adds less than 30 minutes of additional computation to 12 of the 26 studied classification techniques. While widely-used classification techniques like random forest and support vector machines are not optimization-sensitive, traditionally overlooked techniques like C5.0 and neural networks can actually outperform widely-used techniques after optimization is applied. This highlights the importance of exploring the parameter space when using parameter-sensitive classification techniques.},
	number = {7},
	journal = {IEEE Transactions on Software Engineering},
	author = {Tantithamthavorn, Chakkrit and McIntosh, Shane and Hassan, Ahmed E. and Matsumoto, Kenichi},
	month = jul,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {20 most sensitive parameters, automated parameter optimization, classification techniques, Computational efficiency, Computational modeling, defect prediction models, defect-prone software modules, differential evolution, experimental design, genetic algorithm, grid search, Neural networks, nonoptimized classifiers, optimisation, Optimization, optimized settings, parameter optimization, parameter space, parameter-sensitive classification techniques, pattern classification, Power system stability, Predictive models, random forest, random forests, random search, search-based software engineering, Software, Software defect prediction, software quality, support vector machines},
	pages = {683--711},
	file = {IEEE Xplore Abstract Record:/home/rem/Zotero/storage/9TB3CSBK/8263202.html:text/html;送出的版本:/home/rem/Zotero/storage/6U95DUNN/Tantithamthavorn 等。 - 2019 - The Impact of Automated Parameter Optimization on .pdf:application/pdf}
}

@inproceedings{latoza_maintaining_2006,
	address = {New York, NY, USA},
	series = {{ICSE} '06},
	title = {Maintaining mental models: a study of developer work habits},
	isbn = {978-1-59593-375-1},
	shorttitle = {Maintaining mental models},
	doi = {10.1145/1134285.1134355},
	abstract = {To understand developers' typical tools, activities, and practices and their satisfaction with each, we conducted two surveys and eleven interviews. We found that many problems arose because developers were forced to invest great effort recovering implicit knowledge by exploring code and interrupting teammates and this knowledge was only saved in their memory. Contrary to expectations that email and IM prevent expensive task switches caused by face-to-face interruptions, we found that face-to-face communication enjoys many advantages. Contrary to expectations that documentation makes understanding design rationale easy, we found that current design documents are inadequate. Contrary to expectations that code duplication involves the copy and paste of code snippets, developers reported several types of duplication. We use data to characterize these and other problems and draw implications for the design of tools for their solution.},
	booktitle = {Proceedings of the 28th international conference on {Software} engineering},
	publisher = {Association for Computing Machinery},
	author = {LaToza, Thomas D. and Venolia, Gina and DeLine, Robert},
	month = may,
	year = {2006},
	keywords = {agile software development, code duplication, code ownership, communication, debugging, interruptions},
	pages = {492--501}
}

@article{__2016-1,
	title = {软件缺陷预测中基于聚类分析的特征选择方法},
	volume = {46},
	issn = {1674-7267},
	abstract = {软件缺陷预测通过挖掘软件历史仓库,构建缺陷预测模型来预测出被测项目内的潜在缺陷程序模块.但有时候搜集到的缺陷预测数据集中含有的冗余特征和无关特征会影响到缺陷预测模型的性能.提出一种基于聚类分析的特征选择方法 FECAR.具体来说,首先基于特征之间的关联性(即FFC),将已有特征进行聚类分析.随后基于特征与类标间的相关性(即FCR),对每个簇中的特征从高到低进行排序并选出指定数量的特征.在实证研究中,借助对称不确定性(symmetric uncertainty)来计算FFC,借助信息增益(information gain)、卡方值(chi-square)或Relief F来计算FCR.以Ecli...},
	language = {中文;},
	number = {09},
	journal = {中国科学:信息科学},
	author = {刘, 望舒 and 陈, 翔 and 顾, 庆 and 刘, 树龙 and 陈, 道蓄},
	year = {2016},
	keywords = {软件质量保障, 特征选择, 缺陷预测, 数据挖掘, 聚类分析, cluster analysis, data mining, defect prediction, feature selection, software quality assurance},
	pages = {1298--1320}
}

@misc{noauthor__nodate-3,
	title = {软件缺陷预测中数据预处理技术研究 - 中国优秀硕士学位论文全文数据库},
	file = {软件缺陷预测中数据预处理技术研究 - 中国优秀硕士学位论文全文数据库:/home/rem/Zotero/storage/F3AMZ8ET/detail.html:text/html}
}

@misc{noauthor__nodate-4,
	title = {软件缺陷预测中数据预处理机制的研究与系统构建 - 中国优秀硕士学位论文全文数据库},
	file = {软件缺陷预测中数据预处理机制的研究与系统构建 - 中国优秀硕士学位论文全文数据库:/home/rem/Zotero/storage/FSEZQB8Q/detail.html:text/html}
}

@article{__nodate,
	title = {面向航天型号软件缺陷预测的属性选择方法},
	volume = {22},
	language = {cn},
	number = {10},
	journal = {计算机测量与控制},
	author = {解维奇 and 蔡远文 and 程龙 and 赵乙镔},
	pages = {3439--34413447},
	file = {Snapshot:/home/rem/Zotero/storage/DW7KR5D2/view_abstract.html:text/html}
}

@article{__2017-3,
	title = {基于组合机器学习算法的软件缺陷预测模型},
	volume = {54},
	issn = {1000-1239},
	doi = {10.7544/issn1000-1239.2017.20151052},
	language = {zh},
	number = {3},
	journal = {计算机研究与发展},
	author = {傅艺绮, 董威 and Fu Yiqi, Dong Wei},
	month = mar,
	year = {2017},
	pages = {633},
	file = {Full Text PDF:/home/rem/Zotero/storage/7924N6IS/傅艺绮 與 Fu Yiqi - 2017 - 基于组合机器学习算法的软件缺陷预测模型.pdf:application/pdf;Snapshot:/home/rem/Zotero/storage/7XT3V4IP/issn1000-1239.2017.html:text/html}
}

@article{krishnan_decision_2004,
	title = {A {Decision} {Model} for {Software} {Maintenance}},
	volume = {15},
	doi = {10.1287/isre.1040.0037},
	abstract = {In this paper we address the problem of increasing software maintenance costs in a custom software development environment, and develop a stochastic decision model for the maintenance of information systems. Based on this modeling framework, we derive an optimal decision rule for software systems maintenance, and present sensitivity analysis of the optimal policy. We illustrate an application of this model to a large telecommunications switching software system, and present sensitivity analysis of the optimal state for major upgrade derived from our model. Our modeling framework also allows for computing the expected time to perform major upgrade to software systems.},
	journal = {Information Systems Research},
	author = {Krishnan, M. and Mukhopadhyay, Tridas and Kriebel, Charles},
	month = dec,
	year = {2004},
	pages = {396--412},
	file = {Full Text PDF:/home/rem/Zotero/storage/TFEEMX8F/Krishnan 等。 - 2004 - A Decision Model for Software Maintenance.pdf:application/pdf}
}

@article{__nodate-1,
	title = {基于迁移学习的软件缺陷预测},
	volume = {44},
	issn = {1022-4653},
	doi = {10.3969/j.issn.0372-2112.2016.01.017},
	language = {cn},
	number = {1},
	journal = {电子学报},
	author = {程铭, 毋国庆 and CHENG Ming, WU Guo-qing},
	pages = {115--122},
	file = {Full Text PDF:/home/rem/Zotero/storage/XMVVU4FK/程铭 與 CHENG Ming - 基于迁移学习的软件缺陷预测.pdf:application/pdf;Snapshot:/home/rem/Zotero/storage/L58RJVGN/abstract9333.html:text/html}
}

@article{__2015,
	title = {基于元学习的软件缺陷预测推荐方法},
	volume = {10},
	issn = {1673-5692},
	abstract = {基于机器学习的分类算法已被广泛地应用于预测软件缺陷。然而,软件缺陷数据的多样化,导致单一分类算法难以在所有的软件缺陷预测过程中均获得最优的分类性能,即不同的数据集上最适用的分类算法也不尽相同。本文提出了一种基于元学习和实例学习的软件缺陷预测算法推荐方法。该方法仅依据待预测软件缺陷数据的特征,为其推荐最适用的分类算法。},
	language = {中文;},
	number = {06},
	journal = {中国电子科学研究院学报},
	author = {程, 俊 and 张, 雪莹 and 李, 瑞贤},
	year = {2015},
	keywords = {元学习, 实例学习, 软件缺陷预测, 算法推荐, algorithm recommendation, instance-based learning, meta-learning, software defect prediction},
	pages = {620--627}
}

@article{__nodate-2,
	title = {基于字典学习的软件缺陷检测算法},
	volume = {36},
	issn = {1001-9081},
	doi = {10.11772/j.issn.1001-9081.2016.09.2486},
	language = {cn},
	number = {9},
	journal = {计算机应用},
	author = {张蕾, 朱义鑫 and ZHANG Lei, ZHU Yixin},
	pages = {2486--2491},
	file = {Full Text PDF:/home/rem/Zotero/storage/NZHYNNW9/张蕾 與 ZHANG Lei - 基于字典学习的软件缺陷检测算法.pdf:application/pdf;Snapshot:/home/rem/Zotero/storage/X78JQZF5/abstract19701.html:text/html}
}

@article{dambros_evaluating_2012,
	title = {Evaluating defect prediction approaches: a benchmark and an extensive comparison},
	volume = {17},
	issn = {1573-7616},
	shorttitle = {Evaluating defect prediction approaches},
	doi = {10.1007/s10664-011-9173-9},
	abstract = {Reliably predicting software defects is one of the holy grails of software engineering. Researchers have devised and implemented a plethora of defect/bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches. We present a benchmark for defect prediction, in the form of a publicly available dataset consisting of several software systems, and provide an extensive comparison of well-known bug prediction approaches, together with novel approaches we devised. We evaluate the performance of the approaches using different performance indicators: classification of entities as defect-prone or not, ranking of the entities, with and without taking into account the effort to review an entity. We performed three sets of experiments aimed at (1) comparing the approaches across different systems, (2) testing whether the differences in performance are statistically significant, and (3) investigating the stability of approaches across different learners. Our results indicate that, while some approaches perform better than others in a statistically significant manner, external validity in defect prediction is still an open problem, as generalizing results to different contexts/learners proved to be a partially unsuccessful endeavor.},
	language = {en},
	number = {4},
	journal = {Empir Software Eng},
	author = {D’Ambros, Marco and Lanza, Michele and Robbes, Romain},
	month = aug,
	year = {2012},
	pages = {531--577}
}

@article{chen_multi_2018,
	title = {{MULTI}: {Multi}-objective effort-aware just-in-time software defect prediction},
	volume = {93},
	issn = {0950-5849},
	shorttitle = {{MULTI}},
	doi = {10.1016/j.infsof.2017.08.004},
	abstract = {Context: Just-in-time software defect prediction (JIT-SDP) aims to conduct defect prediction on code changes, which have finer granularity. A recent study by Yang et al. has shown that there exist some unsupervised methods, which are comparative to supervised methods in effort-aware JIT-SDP. Objective: However, we still believe that supervised methods should have better prediction performance since they effectively utilize the gathered defect prediction datasets. Therefore we want to design a new supervised method for JIT-SDP with better performance. Method: In this article, we propose a multi-objective optimization based supervised method MULTI to build JIT-SDP models. In particular, we formalize JIT-SDP as a multi-objective optimization problem. One objective is designed to maximize the number of identified buggy changes and another object is designed to minimize the efforts in software quality assurance activities. There exists an obvious conflict between these two objectives. MULTI uses logistic regression to build the models and uses NSGA-II to generate a set of non-dominated solutions, which each solution denotes the coefficient vector for the logistic regression. Results: We design and conduct a large-scale empirical studies to compare MULTI with 43 state-of-the-art supervised and unsupervised methods under the three commonly used performance evaluation scenarios: cross-validation, cross-project-validation, and timewise-cross-validation. Based on six open-source projects with 227,417 changes in total, our experimental results show that MULTI can perform significantly better than all of the state-of-the-art methods when considering ACC and POPT performance metrics. Conclusion: By using multi-objective optimization, MULTI can perform significantly better than the state-of-the-art supervised and unsupervised methods in the three performance evaluation scenarios. The results confirm that supervised methods are still promising in effort-aware JIT-SDP.},
	language = {en},
	journal = {Information and Software Technology},
	author = {Chen, Xiang and Zhao, Yingquan and Wang, Qiuping and Yuan, Zhidan},
	month = jan,
	year = {2018},
	keywords = {Empirical studies, Just-in-time defect prediction, Multi-objective optimization, Search based software engineering},
	pages = {1--13},
	file = {ScienceDirect Snapshot:/home/rem/Zotero/storage/S8XQV3X9/authorization.html:text/html;ScienceDirect Full Text PDF:/home/rem/Zotero/storage/97Y9U68E/Chen 等。 - 2018 - MULTI Multi-objective effort-aware just-in-time s.pdf:application/pdf}
}

@misc{noauthor__nodate-5,
	title = {基于半监督学习的软件缺陷预测方法研究 - 中国优秀硕士学位论文全文数据库},
	file = {基于半监督学习的软件缺陷预测方法研究 - 中国优秀硕士学位论文全文数据库:/home/rem/Zotero/storage/8KDSTW9C/detail.html:text/html}
}

@article{__2020-1,
	title = {基于半监督学习的多源软件缺陷预测模型 {Multi}-{Source} {Software} {Defect} {Prediction} {Model} {Based} on {Semi}-{Supervised} {Learning}},
	volume = {9},
	doi = {10.12677/SEA.2020.92014},
	abstract = {本文研究在不同的软件项目之间，建立通用软件缺陷预测模型的方法。通过分析多源软件的项目信息，本文设计了25维软件},
	language = {en},
	journal = {Software Engineering and Applications},
	author = {于龙海 and 吴晓鸰 and 捷, 凌 and 许遵鸿},
	month = apr,
	year = {2020},
	note = {Publisher: 中文学术期刊,汉斯出版社,Hans Publishers},
	pages = {116},
	file = {Snapshot:/home/rem/Zotero/storage/XRYVY4TV/35163.html:text/html}
}

@article{hall_systematic_2012,
	title = {A {Systematic} {Literature} {Review} on {Fault} {Prediction} {Performance} in {Software} {Engineering}},
	volume = {38},
	issn = {1939-3520},
	doi = {10.1109/TSE.2011.103},
	abstract = {Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.},
	number = {6},
	journal = {IEEE Transactions on Software Engineering},
	author = {Hall, Tracy and Beecham, Sarah and Bowes, David and Gray, David and Counsell, Steve},
	month = nov,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Analytical models, Bayes methods, Context modeling, contextual information, cost reduction, Data models, Fault diagnosis, fault prediction models, fault prediction performance, fault prediction study, feature selection, independent variables, logistic regression, methodological information, naive Bayes, Predictive models, predictive performance, regression analysis, reliable methodology, simple modeling techniques, software engineering, software fault prediction, software fault tolerance, software quality, Software testing, systematic literature review, Systematic literature review, Systematics},
	pages = {1276--1304},
	file = {IEEE Xplore Abstract Record:/home/rem/Zotero/storage/J7LEBTFA/6035727.html:text/html;送出的版本:/home/rem/Zotero/storage/7D545Y6W/Hall 等。 - 2012 - A Systematic Literature Review on Fault Prediction.pdf:application/pdf}
}

@inproceedings{pinzger_can_2008,
	address = {New York, NY, USA},
	series = {{SIGSOFT} '08/{FSE}-16},
	title = {Can developer-module networks predict failures?},
	isbn = {978-1-59593-995-1},
	doi = {10.1145/1453101.1453105},
	abstract = {Software teams should follow a well defined goal and keep their work focused. Work fragmentation is bad for efficiency and quality. In this paper we empirically investigate the relationship between the fragmentation of developer contributions and the number of post-release failures. Our approach is to represent developer contributions with a developer-module network that we call contribution network. We use network centrality measures to measure the degree of fragmentation of developer contributions. Fragmentation is determined by the centrality of software modules in the contribution network. Our claim is that central software modules are more likely to be failure-prone than modules located in surrounding areas of the network. We analyze this hypothesis by exploring the network centrality of Microsoft Windows Vista binaries using several network centrality measures as well as linear and logistic regression analysis. In particular, we investigate which centrality measures are significant to predict the probability and number of post-release failures. Results of our experiments show that central modules are more failure-prone than modules located in surrounding areas of the network. Results further confirm that number of authors and number of commits are significant predictors for the probability of post-release failures. For predicting the number of post-release failures the closeness centrality measure is most significant.},
	booktitle = {Proceedings of the 16th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of software engineering},
	publisher = {Association for Computing Machinery},
	author = {Pinzger, Martin and Nagappan, Nachiappan and Murphy, Brendan},
	month = nov,
	year = {2008},
	keywords = {developer contribution network, failure prediction, network centrality measures, social network analysis},
	pages = {2--12},
	file = {接受的版本:/home/rem/Zotero/storage/BIW6HTGS/Pinzger 等。 - 2008 - Can developer-module networks predict failures.pdf:application/pdf}
}

@article{__2017-4,
	title = {基于机器学习的软件缺陷识别的必要性},
	volume = {13},
	number = {9},
	journal = {电脑知识与技术：学术交流},
	author = {韩宏峰 and 罗羿隆 and 相克磊 and 徐毅蒙},
	year = {2017},
	pages = {185--186},
	file = {Snapshot:/home/rem/Zotero/storage/7FBPPYH6/673430730.html:text/html}
}

@article{mahmood_reproducibility_2018,
	title = {Reproducibility and replicability of software defect prediction studies},
	volume = {99},
	issn = {0950-5849},
	doi = {10.1016/j.infsof.2018.02.003},
	abstract = {Context: Replications are an important part of scienti
@misc{noauthor_pdf_nodate-1,
	title = {({PDF}) {A} {Large}-{Scale} {Empirical} {Study} of {Just}-in-{Time} {Quality} {Assurance}},
	file = {(PDF) A Large-Scale Empirical Study of Just-in-Time Quality Assurance:/home/rem/Zotero/storage/DHN87IH2/260648765_A_Large-Scale_Empirical_Study_of_Just-in-Time_Quality_Assurance.html:text/html}
}fic disciplines. Replications test the credibility of original studies and can separate true results from those that are unreliable. Objective: In this paper we investigate the replication of defect prediction studies and identify the characteristics of replicated studies. We further assess how defect prediction replications are performed and the consistency of replication findings. Method: Our analysis is based on tracking the replication of 208 defect prediction studies identified by a highly cited Systematic Literature Review (SLR) [1]. We identify how often each of these 208 studies has been replicated and determine the type of replication carried out. We identify quality, citation counts, publication venue, impact factor, and data availability from all 208 SLR defect prediction papers to see if any of these factors are associated with the frequency with which they are replicated. Results: Only 13 (6\%) of the 208 studies are replicated. Replication seems related to original papers appearing in the Transactions of Software Engineering (TSE) journal. The number of citations an original paper had was also an indicator of replications. In addition, studies conducted using closed source data seems to have more replications than those based on open source data. Where a paper has been replicated, 11 (38\%) out of 29 studies revealed different results to the original study. Conclusion: Very few defect prediction studies are replicated. The lack of replication means that it remains unclear how reliable defect prediction is. We provide practical steps for improving the state of replication.},
	language = {en},
	journal = {Information and Software Technology},
	author = {Mahmood, Zaheed and Bowes, David and Hall, Tracy and Lane, Peter C. R. and Petrić, Jean},
	month = jul,
	year = {2018},
	keywords = {Replication, Reproducibility, Software defect prediction},
	pages = {148--163}
}

@inproceedings{nam_clami_2015,
	title = {{CLAMI}: {Defect} {Prediction} on {Unlabeled} {Datasets} ({T})},
	shorttitle = {{CLAMI}},
	doi = {10.1109/ASE.2015.56},
	abstract = {Defect prediction on new projects or projects with limited historical data is an interesting problem in software engineering. This is largely because it is difficult to collect defect information to label a dataset for training a prediction model. Cross-project defect prediction (CPDP) has tried to address this problem by reusing prediction models built by other projects that have enough historical data. However, CPDP does not always build a strong prediction model because of the different distributions among datasets. Approaches for defect prediction on unlabeled datasets have also tried to address the problem by adopting unsupervised learning but it has one major limitation, the necessity for manual effort. In this study, we propose novel approaches, CLA and CLAMI, that show the potential for defect prediction on unlabeled datasets in an automated manner without need for manual effort. The key idea of the CLA and CLAMI approaches is to label an unlabeled dataset by using the magnitude of metric values. In our empirical study on seven open-source projects, the CLAMI approach led to the promising prediction performances, 0.636 and 0.723 in average f-measure and AUC, that are comparable to those of defect prediction based on supervised learning.},
	booktitle = {2015 30th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Nam, Jaechang and Kim, Sunghun},
	month = nov,
	year = {2015},
	keywords = {CLA approach, CLAMI approach, cross-project defect prediction, Data models, defect information collection, Manuals, Measurement, Predictive models, Software, software engineering, software fault tolerance, software quality, supervised learning, Supervised learning, Training, unsupervised learning},
	pages = {452--463},
	file = {IEEE Xplore Abstract Record:/home/rem/Zotero/storage/3VZIJS6C/7372033.html:text/html}
}

@article{__2016-2,
	title = {静态软件缺陷预测方法研究},
	volume = {27},
	issn = {1000-9825},
	abstract = {静态软件缺陷预测是软件工程数据挖掘领域中的一个研究热点.通过分析软件代码或开发过程,设计出与软件缺陷相关的度量元;随后,通过挖掘软件历史仓库来创建缺陷预测数据集,旨在构建出缺陷预测模型,以预测出被测项目内的潜在缺陷程序模块,最终达到优化测试资源分配和提高软件产品质量的目的.对近些年来国内外学者在该研究领域取得的成果进行了系统的总结.首先,给出了研究框架并识别出了影响缺陷预测性能的3个重要影响因素:度量元的设定、缺陷预测模型的构建方法和缺陷预测数据集的相关问题;接着,依次总结了这3个影响因素的已有研究成果;随后,总结了一类特殊的软件缺陷预测问题(即,基于代码修改的缺陷预测)的已有研究工作;最后,...},
	language = {中文;},
	number = {01},
	journal = {软件学报},
	author = {陈, 翔 and 顾, 庆 and 刘, 望舒 and 刘, 树龙 and 倪, 超},
	year = {2016},
	keywords = {机器学习, 软件质量保障, 软件度量元, 软件缺陷预测, 数据集预处理, data preprocessing, machine learning, software defect prediction, software metrics, software quality assurance},
	pages = {1--25}
}

@misc{__2019,
	title = {基于特征迁移和实例迁移的跨项目缺陷预测方法},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	abstract = {在实际软件开发中，需要进行缺陷预测的项目可能是一个新启动项目，或者这个项目的历史训练数据较为稀缺.一种解决方案是利用其他项目（即源项目）已搜集的训练数据来构建模型，并完成对当前项目（即目标项目）的预测.但不同项目的数据集间会存在较大的分布差异性.针对该问题，从特征迁移和实例迁移角度出发，提出了一种两阶段跨项目缺陷预测方法FeCTrA.具体来说，在特征迁移阶段，该方法借助聚类分析选出源项目与目标项目之间具有高分布相似度的特征；在实例迁移阶段，该方法基于TrAdaBoost方法，借助目标项目中的少量已标注实例，从源项目中选出与这些已标注实例分布相近的实例.为了验证FeCTrA方法的有效性，选择Relink数据集和AEEEM数据集作为评测对象，以\textit{F}1作为评测指标.首先，FeCTrA方法的预测性能要优于仅考虑特征迁移阶段或实例迁移阶段的单阶段方法；其次，与经典的跨项目缺陷预测方法TCA+、Peters过滤法、Burak过滤法以及DCPDP法相比，FeCTrA方法的预测性能在Relink数据集上可以分别提升23\%、7.2\%、9.8\%和38.2\%，在AEEEM数据集上可以分别提升96.5\%、108.5\%、103.6\%和107.9\%；最后，分析了FeCTrA方法内的影响因素对预测性能的影响，从而为有效使用FeCTrA方法提供了指南.},
	language = {cn},
	author = {倪超 and Chao, N. I. and 陈翔 and Xiang, CHEN and 刘望舒 and Wang-Shu, L. I. U. and 顾庆 and Qing, G. U. and 黄启国 and Qi-Guo, HUANG and 李娜 and Na, L. I.},
	month = may,
	year = {2019},
	doi = {10.13328/j.cnki.jos.005712},
	note = {Publisher: 软件学报},
	file = {Snapshot:/home/rem/Zotero/storage/87CDVRJQ/5712.html:text/html}
}

@article{__2017-5,
	title = {数据驱动的软件缺陷预测研究综述},
	volume = {45},
	issn = {0372-2112},
	abstract = {数据驱动的软件缺陷预测是提高软件测试效率、保证软件可靠性的重要途径之一,近几年已成为实证软件工程的研究热点.首先介绍了数据驱动软件缺陷预测的研究背景;然后总结了已有软件缺陷数据属性度量方法的特点,并按照软件开发中缺陷预测的使用场景,以数据来源为主线从基于版本内数据、跨版本数据和跨项目数据实现缺陷预测三个方面对近10年(2005{\textasciitilde}2015)已有的研究工作进行分类归纳和比较;最后对该领域未来的研究趋势进行了展望.},
	language = {中文;},
	number = {04},
	journal = {电子学报},
	author = {李, 勇 and 黄, 志球 and 王, 勇 and 房, 丙午},
	year = {2017},
	keywords = {机器学习, 软件度量, 软件缺陷预测, 数据驱动, data driven, machine learning, software defects prediction, software metrics},
	pages = {982--988}
}

@article{kamei_large-scale_2013,
	title = {A {Large}-{Scale} {Empirical} {Study} of {Just}-in-{Time} {Quality} {Assurance}},
	volume = {39},
	doi = {10.1109/TSE.2012.70},
	abstract = {Defect prediction models are a well-known technique for identifying defect-prone files or packages such that practitioners can allocate their quality assurance efforts (e.g., testing and code reviews). However, once the critical files or packages have been identified, developers still need to spend considerable time drilling down to the functions or even code snippets that should be reviewed or tested. This makes the approach too time consuming and impractical for large software systems. Instead, we consider defect prediction models that focus on identifying defect-prone (“risky”) software changes instead of files or packages. We refer to this type of quality assurance activity as “Just-In-Time Quality Assurance,” because developers can review and test these risky changes while they are still fresh in their minds (i.e., at check-in time). To build a change risk model, we use a wide range of factors based on the characteristics of a software change, such as the number of added lines, and developer experience. A large-scale study of six open source and five commercial projects from multiple domains shows that our models can predict whether or not a change will lead to a defect with an average accuracy of 68 percent and an average recall of 64 percent. Furthermore, when considering the effort needed to review changes, we find that using only 20 percent of the effort it would take to inspect all changes, we can identify 35 percent of all defect-inducing changes. Our findings indicate that “Just-In-Time Quality Assurance” may provide an effort-reducing way to focus on the most risky changes and thus reduce the costs of developing high-quality software.},
	journal = {Software Engineering, IEEE Transactions on},
	author = {Kamei, Yasutaka and Shihab, Emad and Adams, Bram and Hassan, Ahmed E. and Mockus, Audris and Sinha, Anand and Ubayashi, Naoyasu},
	month = jun,
	year = {2013},
	pages = {757--773},
	file = {Full Text PDF:/home/rem/Zotero/storage/4QRUGFEI/Kamei 等。 - 2013 - A Large-Scale Empirical Study of Just-in-Time Qual.pdf:application/pdf}
}

@article{menzies_defect_2010,
	title = {Defect prediction from static code features: current results, limitations, new approaches},
	volume = {17},
	issn = {1573-7535},
	shorttitle = {Defect prediction from static code features},
	url = {https://doi.org/10.1007/s10515-010-0069-5},
	doi = {10.1007/s10515-010-0069-5},
	abstract = {Building quality software is expensive and software quality assurance (QA) budgets are limited. Data miners can learn defect predictors from static code features which can be used to control QA resources; e.g. to focus on the parts of the code predicted to be more defective.},
	language = {en},
	number = {4},
	urldate = {2020-10-09},
	journal = {Autom Softw Eng},
	author = {Menzies, Tim and Milton, Zach and Turhan, Burak and Cukic, Bojan and Jiang, Yue and Bener, Ayşe},
	month = dec,
	year = {2010},
	pages = {375--407}
}

@inproceedings{jiang_personalized_2013,
	title = {Personalized defect prediction},
	doi = {10.1109/ASE.2013.6693087},
	abstract = {Many defect prediction techniques have been proposed. While they often take the author of the code into consideration, none of these techniques build a separate prediction model for each developer. Different developers have different coding styles, commit frequencies, and experience levels, causing different defect patterns. When the defects of different developers are combined, such differences are obscured, hurting prediction performance. This paper proposes personalized defect prediction-building a separate prediction model for each developer to predict software defects. As a proof of concept, we apply our personalized defect prediction to classify defects at the file change level. We evaluate our personalized change classification technique on six large software projects written in C and Java-the Linux kernel, PostgreSQL, Xorg, Eclipse, Lucene and Jackrabbit. Our personalized approach can discover up to 155 more bugs than the traditional change classification (210 versus 55) if developers inspect the top 20\% lines of code that are predicted buggy. In addition, our approach improves the F1-score by 0.01-0.06 compared to the traditional change classification.},
	booktitle = {2013 28th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Jiang, Tian and Tan, Lin and Kim, Sunghun},
	month = nov,
	year = {2013},
	keywords = {C software projects, Change classification, coding styles, commit frequencies, Computer bugs, different defect patterns, Eclipse, experience levels, Feature extraction, Jackrabbit, Java, java software projects, Linux, Linux kernel, Lucene, machine learning, Mars, personalized defect prediction, PostgreSQL, Predictive models, program compilers, separate prediction model, software defect prediction, software reliability, Syntactics, Training, Vectors, Xorg},
	pages = {279--289},
	file = {IEEE Xplore Abstract Record:/home/rem/Zotero/storage/E3GLHDP3/6693087.html:text/html;送出的版本:/home/rem/Zotero/storage/VSJTNM99/Jiang 等。 - 2013 - Personalized defect prediction.pdf:application/pdf}
}